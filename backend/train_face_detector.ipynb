{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a81132",
   "metadata": {},
   "source": [
    "# üéì SVM+ORB Face Detector Training\n",
    "## Classical Computer Vision Face Detection Model\n",
    "\n",
    "Train your own face detector using:\n",
    "- **ORB** (Oriented FAST + Rotated BRIEF) for feature extraction\n",
    "- **Bag of Visual Words (BoVW)** with K-means clustering\n",
    "- **Linear SVM** for classification\n",
    "\n",
    "### üì¶ What You'll Get:\n",
    "1. `svm_model.pkl` - Trained SVM classifier\n",
    "2. `bovw_encoder.pkl` - K-means codebook (BoVW encoder)\n",
    "3. `scaler.pkl` - Feature scaler\n",
    "\n",
    "### üìÇ Dataset Structure Required:\n",
    "```\n",
    "dataset.zip\n",
    "‚îú‚îÄ‚îÄ faces/          (positive samples - face images)\n",
    "‚îî‚îÄ‚îÄ non_faces/      (negative samples - non-face images)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec98367",
   "metadata": {},
   "source": [
    "## üì• Step 1: Setup & Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29de189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install opencv-python scikit-learn scikit-image joblib numpy matplotlib seaborn tqdm -q\n",
    "\n",
    "print(\"‚úì Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc26358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Libraries imported\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your dataset.zip file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Please upload your dataset.zip file (contains 'faces/' and 'non_faces/' folders)\")\n",
    "print(\"‚è≥ Waiting for upload...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(\"\\n‚úì Upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset\n",
    "import zipfile\n",
    "\n",
    "zip_filename = list(uploaded.keys())[0]\n",
    "print(f\"üì¶ Extracting {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')\n",
    "\n",
    "print(\"‚úì Extraction complete\")\n",
    "\n",
    "# Verify structure\n",
    "faces_dir = 'data/faces'\n",
    "non_faces_dir = 'data/non_faces'\n",
    "\n",
    "if os.path.exists(faces_dir):\n",
    "    num_faces = len(os.listdir(faces_dir))\n",
    "    print(f\"‚úì Found {num_faces} face images\")\n",
    "else:\n",
    "    print(\"‚úó 'faces/' folder not found!\")\n",
    "\n",
    "if os.path.exists(non_faces_dir):\n",
    "    num_non_faces = len(os.listdir(non_faces_dir))\n",
    "    print(f\"‚úì Found {num_non_faces} non-face images\")\n",
    "else:\n",
    "    print(\"‚úó 'non_faces/' folder not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de48e5",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Step 2: Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Show face samples\n",
    "face_files = [f for f in os.listdir(faces_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:5]\n",
    "for i, fname in enumerate(face_files):\n",
    "    img = cv2.imread(os.path.join(faces_dir, fname))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f'Face {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Show non-face samples\n",
    "non_face_files = [f for f in os.listdir(non_faces_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:5]\n",
    "for i, fname in enumerate(non_face_files):\n",
    "    img = cv2.imread(os.path.join(non_faces_dir, fname))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].set_title(f'Non-Face {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"  Faces: {num_faces}\")\n",
    "print(f\"  Non-faces: {num_non_faces}\")\n",
    "print(f\"  Total: {num_faces + num_non_faces}\")\n",
    "print(f\"  Balance: {num_faces/(num_faces + num_non_faces)*100:.1f}% faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5945ff",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf42793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    # ORB parameters\n",
    "    'orb_max_keypoints': 500,\n",
    "    'orb_scale_factor': 1.2,\n",
    "    'orb_n_levels': 8,\n",
    "    'orb_patch_size': 31,\n",
    "    \n",
    "    # Image preprocessing\n",
    "    'target_size': (128, 128),  # Resize all images to this size\n",
    "    \n",
    "    # BoVW parameters\n",
    "    'bovw_k': 256,  # Number of visual words (clusters)\n",
    "    'bovw_max_descriptors': 200000,  # Max descriptors for k-means training\n",
    "    \n",
    "    # SVM parameters\n",
    "    'svm_C': [0.1, 1.0, 10.0],  # Regularization parameter (will try all)\n",
    "    'svm_max_iter': 10000,\n",
    "    \n",
    "    # Data split\n",
    "    'train_ratio': 0.70,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc71a8",
   "metadata": {},
   "source": [
    "## üîç Step 4: Extract ORB Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create(\n",
    "    nfeatures=CONFIG['orb_max_keypoints'],\n",
    "    scaleFactor=CONFIG['orb_scale_factor'],\n",
    "    nlevels=CONFIG['orb_n_levels'],\n",
    "    edgeThreshold=CONFIG['orb_patch_size'],\n",
    "    patchSize=CONFIG['orb_patch_size'],\n",
    "    WTA_K=2,\n",
    "    scoreType=cv2.ORB_HARRIS_SCORE\n",
    ")\n",
    "\n",
    "print(f\"‚úì ORB detector initialized\")\n",
    "print(f\"  Max keypoints: {CONFIG['orb_max_keypoints']}\")\n",
    "print(f\"  Scale factor: {CONFIG['orb_scale_factor']}\")\n",
    "print(f\"  Pyramid levels: {CONFIG['orb_n_levels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55234ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_path, target_size):\n",
    "    \"\"\"Load image and preprocess for ORB feature extraction\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize\n",
    "    gray = cv2.resize(gray, target_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Histogram equalization\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "    \n",
    "    return gray\n",
    "\n",
    "\n",
    "def extract_orb_descriptors(img, orb_detector):\n",
    "    \"\"\"Extract ORB descriptors from image\"\"\"\n",
    "    keypoints, descriptors = orb_detector.detectAndCompute(img, None)\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f417a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ORB descriptors from all images\n",
    "print(\"üîç Extracting ORB features from all images...\\n\")\n",
    "\n",
    "all_descriptors = []  # For k-means training\n",
    "descriptors_per_image = []  # Store descriptors for each image\n",
    "labels = []  # Labels: 1=face, 0=non-face\n",
    "filepaths = []  # Keep track of file paths\n",
    "\n",
    "# Process face images (label = 1)\n",
    "print(\"Processing face images...\")\n",
    "face_files = [f for f in os.listdir(faces_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "for fname in tqdm(face_files, desc=\"Faces\"):\n",
    "    img_path = os.path.join(faces_dir, fname)\n",
    "    img = load_and_preprocess_image(img_path, CONFIG['target_size'])\n",
    "    \n",
    "    if img is not None:\n",
    "        descriptors = extract_orb_descriptors(img, orb)\n",
    "        \n",
    "        if descriptors is not None and len(descriptors) > 0:\n",
    "            descriptors_per_image.append(descriptors)\n",
    "            labels.append(1)  # Face\n",
    "            filepaths.append(img_path)\n",
    "            \n",
    "            # Add to global descriptor pool\n",
    "            all_descriptors.append(descriptors)\n",
    "\n",
    "# Process non-face images (label = 0)\n",
    "print(\"\\nProcessing non-face images...\")\n",
    "non_face_files = [f for f in os.listdir(non_faces_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "for fname in tqdm(non_face_files, desc=\"Non-faces\"):\n",
    "    img_path = os.path.join(non_faces_dir, fname)\n",
    "    img = load_and_preprocess_image(img_path, CONFIG['target_size'])\n",
    "    \n",
    "    if img is not None:\n",
    "        descriptors = extract_orb_descriptors(img, orb)\n",
    "        \n",
    "        if descriptors is not None and len(descriptors) > 0:\n",
    "            descriptors_per_image.append(descriptors)\n",
    "            labels.append(0)  # Non-face\n",
    "            filepaths.append(img_path)\n",
    "            \n",
    "            # Add to global descriptor pool\n",
    "            all_descriptors.append(descriptors)\n",
    "\n",
    "print(f\"\\n‚úì Feature extraction complete\")\n",
    "print(f\"  Total images processed: {len(descriptors_per_image)}\")\n",
    "print(f\"  Faces: {sum(labels)}\")\n",
    "print(f\"  Non-faces: {len(labels) - sum(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268ef99",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Build BoVW Codebook (K-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare descriptors for k-means\n",
    "print(\"üìä Preparing descriptors for k-means clustering...\\n\")\n",
    "\n",
    "# Stack all descriptors\n",
    "all_descriptors_stacked = np.vstack(all_descriptors)\n",
    "print(f\"Total descriptors: {len(all_descriptors_stacked):,}\")\n",
    "\n",
    "# Sample descriptors if too many\n",
    "max_desc = CONFIG['bovw_max_descriptors']\n",
    "if len(all_descriptors_stacked) > max_desc:\n",
    "    print(f\"Sampling {max_desc:,} descriptors for k-means...\")\n",
    "    np.random.seed(CONFIG['random_seed'])\n",
    "    indices = np.random.choice(len(all_descriptors_stacked), max_desc, replace=False)\n",
    "    descriptors_for_kmeans = all_descriptors_stacked[indices]\n",
    "else:\n",
    "    descriptors_for_kmeans = all_descriptors_stacked\n",
    "\n",
    "print(f\"‚úì Using {len(descriptors_for_kmeans):,} descriptors for k-means\")\n",
    "print(f\"  Descriptor shape: {descriptors_for_kmeans.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train K-means clustering (BoVW codebook)\n",
    "print(f\"\\nüéØ Training K-means with k={CONFIG['bovw_k']}...\")\n",
    "print(\"‚è≥ This may take a few minutes...\\n\")\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=CONFIG['bovw_k'],\n",
    "    random_state=CONFIG['random_seed'],\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "kmeans.fit(descriptors_for_kmeans)\n",
    "\n",
    "print(f\"\\n‚úì K-means training complete\")\n",
    "print(f\"  Number of clusters (visual words): {kmeans.n_clusters}\")\n",
    "print(f\"  Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"  Iterations: {kmeans.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8b1c2",
   "metadata": {},
   "source": [
    "## üìä Step 6: Encode Images to BoVW Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e166e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_to_bovw(descriptors, kmeans_model, k, normalize='l2'):\n",
    "    \"\"\"Encode ORB descriptors to BoVW histogram\"\"\"\n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        # Return uniform histogram for images with no keypoints\n",
    "        return np.ones(k, dtype=np.float32) / k\n",
    "    \n",
    "    # Predict visual words\n",
    "    visual_words = kmeans_model.predict(descriptors)\n",
    "    \n",
    "    # Build histogram\n",
    "    histogram = np.bincount(visual_words, minlength=k).astype(np.float32)\n",
    "    \n",
    "    # Normalize\n",
    "    if normalize == 'l1':\n",
    "        norm = np.sum(histogram)\n",
    "        if norm > 0:\n",
    "            histogram = histogram / norm\n",
    "    elif normalize == 'l2':\n",
    "        norm = np.linalg.norm(histogram)\n",
    "        if norm > 0:\n",
    "            histogram = histogram / norm\n",
    "    \n",
    "    return histogram\n",
    "\n",
    "\n",
    "print(\"‚úì BoVW encoding function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all images to BoVW histograms\n",
    "print(\"üìä Encoding images to BoVW histograms...\\n\")\n",
    "\n",
    "X = []  # Features (BoVW histograms)\n",
    "y = np.array(labels)  # Labels\n",
    "\n",
    "for descriptors in tqdm(descriptors_per_image, desc=\"Encoding\"):\n",
    "    histogram = encode_to_bovw(descriptors, kmeans, CONFIG['bovw_k'], normalize='l2')\n",
    "    X.append(histogram)\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "print(f\"\\n‚úì Encoding complete\")\n",
    "print(f\"  Feature matrix shape: {X.shape}\")\n",
    "print(f\"  Labels shape: {y.shape}\")\n",
    "print(f\"  Features per image: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e16c51",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 7: Split Dataset (70% / 15% / 15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset: 70% train, 15% val, 15% test\n",
    "print(\"‚úÇÔ∏è Splitting dataset...\\n\")\n",
    "\n",
    "# First split: 70% train, 30% temp (val + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.30,\n",
    "    random_state=CONFIG['random_seed'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 50% val, 50% test from temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    random_state=CONFIG['random_seed'],\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"‚úì Dataset split complete\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Total: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Faces: {sum(y_train)}\")\n",
    "print(f\"  Non-faces: {len(y_train) - sum(y_train)}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Total: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Faces: {sum(y_val)}\")\n",
    "print(f\"  Non-faces: {len(y_val) - sum(y_val)}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Total: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Faces: {sum(y_test)}\")\n",
    "print(f\"  Non-faces: {len(y_test) - sum(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cade128",
   "metadata": {},
   "source": [
    "## üîß Step 8: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bca921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler\n",
    "print(\"üîß Scaling features...\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úì Feature scaling complete\")\n",
    "print(f\"  Scaler mean: {scaler.mean_[:5]}...\")\n",
    "print(f\"  Scaler std: {scaler.scale_[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc288aa",
   "metadata": {},
   "source": [
    "## ü§ñ Step 9: Train Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62612c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "print(\"ü§ñ Training Linear SVM with hyperparameter tuning...\\n\")\n",
    "\n",
    "param_grid = {\n",
    "    'C': CONFIG['svm_C']\n",
    "}\n",
    "\n",
    "svm = LinearSVC(\n",
    "    max_iter=CONFIG['svm_max_iter'],\n",
    "    random_state=CONFIG['random_seed'],\n",
    "    dual=False  # Use primal formulation for faster training\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    svm,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n‚úì Training complete\")\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get best model\n",
    "best_svm = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e67925",
   "metadata": {},
   "source": [
    "## üìà Step 10: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8af132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"üìà Evaluating on validation set...\\n\")\n",
    "\n",
    "y_val_pred = best_svm.predict(X_val_scaled)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Accuracy:  {val_accuracy:.4f}\")\n",
    "print(f\"  Precision: {val_precision:.4f}\")\n",
    "print(f\"  Recall:    {val_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {val_f1:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb15456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nüìà Evaluating on test set...\\n\")\n",
    "\n",
    "y_test_pred = best_svm.predict(X_test_scaled)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {test_f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n\" + classification_report(y_test, y_test_pred, target_names=['Non-Face', 'Face']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Face', 'Face'], yticklabels=['Non-Face', 'Face'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Negatives:  {cm[0,0]}\")\n",
    "print(f\"False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}\")\n",
    "print(f\"True Positives:  {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b8645",
   "metadata": {},
   "source": [
    "## üíæ Step 11: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = 'trained_models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Saving models to '{output_dir}/'...\\n\")\n",
    "\n",
    "# Save SVM model\n",
    "svm_path = os.path.join(output_dir, 'svm_model.pkl')\n",
    "joblib.dump(best_svm, svm_path)\n",
    "print(f\"‚úì SVM model saved: {svm_path}\")\n",
    "print(f\"  File size: {os.path.getsize(svm_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Save BoVW encoder (k-means)\n",
    "bovw_path = os.path.join(output_dir, 'bovw_encoder.pkl')\n",
    "joblib.dump(kmeans, bovw_path)\n",
    "print(f\"‚úì BoVW encoder saved: {bovw_path}\")\n",
    "print(f\"  File size: {os.path.getsize(bovw_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = os.path.join(output_dir, 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"‚úì Scaler saved: {scaler_path}\")\n",
    "print(f\"  File size: {os.path.getsize(scaler_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Save training config and metrics\n",
    "config_path = os.path.join(output_dir, 'training_info.txt')\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"SVM+ORB Face Detector Training Results\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Configuration:\\n\")\n",
    "    f.write(\"-\"*60 + \"\\n\")\n",
    "    for key, value in CONFIG.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"Dataset Split:\\n\")\n",
    "    f.write(\"-\"*60 + \"\\n\")\n",
    "    f.write(f\"Training: {len(X_train)} samples\\n\")\n",
    "    f.write(f\"Validation: {len(X_val)} samples\\n\")\n",
    "    f.write(f\"Test: {len(X_test)} samples\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"Test Set Performance:\\n\")\n",
    "    f.write(\"-\"*60 + \"\\n\")\n",
    "    f.write(f\"Accuracy:  {test_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Precision: {test_precision:.4f}\\n\")\n",
    "    f.write(f\"Recall:    {test_recall:.4f}\\n\")\n",
    "    f.write(f\"F1 Score:  {test_f1:.4f}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"‚úì Training info saved: {config_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL MODELS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357ba01",
   "metadata": {},
   "source": [
    "## üì• Step 12: Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cdf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all models for easy download\n",
    "import shutil\n",
    "\n",
    "print(\"üì¶ Creating models.zip for download...\\n\")\n",
    "\n",
    "zip_filename = 'trained_models'\n",
    "shutil.make_archive(zip_filename, 'zip', output_dir)\n",
    "\n",
    "print(f\"‚úì Archive created: {zip_filename}.zip\")\n",
    "print(f\"  Size: {os.path.getsize(zip_filename + '.zip') / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Download\n",
    "print(\"\\nüì• Downloading models.zip...\")\n",
    "files.download(f'{zip_filename}.zip')\n",
    "\n",
    "print(\"\\n‚úÖ Download complete!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Extract trained_models.zip\")\n",
    "print(\"2. Copy these files to your backend/models/ folder:\")\n",
    "print(\"   - svm_model.pkl\")\n",
    "print(\"   - bovw_encoder.pkl\")\n",
    "print(\"   - scaler.pkl\")\n",
    "print(\"3. Run: python face_detector_cli.py webcam --camera 0 --show\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357bc4f",
   "metadata": {},
   "source": [
    "## üìä Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Dataset Size: {len(X)} images\")\n",
    "print(f\"Training Set: {len(X_train)} (70%)\")\n",
    "print(f\"Validation Set: {len(X_val)} (15%)\")\n",
    "print(f\"Test Set: {len(X_test)} (15%)\")\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"  Accuracy:  {test_accuracy*100:.2f}%\")\n",
    "print(f\"  Precision: {test_precision*100:.2f}%\")\n",
    "print(f\"  Recall:    {test_recall*100:.2f}%\")\n",
    "print(f\"  F1 Score:  {test_f1*100:.2f}%\")\n",
    "print(\"\\nModel Details:\")\n",
    "print(f\"  ORB Keypoints: {CONFIG['orb_max_keypoints']}\")\n",
    "print(f\"  BoVW Clusters (k): {CONFIG['bovw_k']}\")\n",
    "print(f\"  SVM C parameter: {best_svm.C}\")\n",
    "print(f\"  Feature dimension: {X.shape[1]}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ Models saved and ready for deployment!\")\n",
    "print(\"üì• Download the trained_models.zip file above\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
